---
title: 爬虫之简介和基本原理
date: 2018-02-12 15:56:37
tags: Python
---
### 爬虫简介
> 爬虫，即网络爬虫，可理解为在网络上爬行的一直蜘蛛，互联网就比作一张大网，而爬虫便是在这张网上爬来爬去的蜘蛛，如果它遇到资源，那么它就会抓取下来。

一句话形容爬虫：一段自定抓取互联网信息的程序

![](http://upload-images.jianshu.io/upload_images/1335668-4f072d4ea842f92f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

爬虫的作用
取互联网数据，为我所用！

![](http://upload-images.jianshu.io/upload_images/1335668-236e7de9711272c6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

简单爬虫架构

![](http://upload-images.jianshu.io/upload_images/1335668-db4d7eb3f4fa72e2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

运行流程

![](http://upload-images.jianshu.io/upload_images/1335668-d6e72bf10890b54f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### URL管理器

#### URL简单介绍

> URL，即统一资源定位符，也就是我们说的网址，统一资源定位符是对可以从互联网上得到的资源的位置和访问方法的一种简洁的表示，是互联网上标准资源的地址。互联网上的每个文件都有一个唯一的URL，它包含的信息指出文件的位置以及浏览器应该怎么处理它。

URL的格式由三部分组成：

* ①第一部分是协议(或称为服务方式)
* ②第二部分是存有该资源的主机IP地址(有时也包括端口号)
* ③第三部分是主机资源的具体地址，如目录和文件名等

爬虫爬取数据时必须要有一个目标的URL才可以获取数据，因此，它是爬虫获取数据的基本依据

URL管理器

> 管理待抓取URL的集合和已抓取的URL集合

目的：防止重复抓取、防止循环抓取

![](http://upload-images.jianshu.io/upload_images/1335668-8107d2782cc66e8f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

#### URL管理器几种常见实现方式

* 内存：Python 内存，set()
* 关系数据库：MySQL
* 缓存数据库：Redis

### 网页下载器
#### Python 中常用网页下载器

* urllib（Python3）
* [Requests](http://cn.python-requests.org/zh_CN/latest/) （很强大的第三方库）

##### urllib 简单使用一

```
import urllib.request
# 核心方法
response = urllib.request.urlopen("http://www.baidu.com")
# 输出请求结果
print(response.read().decode('utf-8'))
```
##### urllib 简单使用二

```
import urllib.request
# 创建request
request = urllib.request.Request("http://www.baidu.com")
# 获取相应体
response = urllib.request.urlopen(request)
# 输出
print(response.read().decode("utf-8"))
```
### 网页解析器

> 从网页中获取有价值数据的工具

图例： 
![](http://upload-images.jianshu.io/upload_images/1335668-c198b576ab63119f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

#### Python 常见网页解析器实现

* 正则表达式：模糊匹配
* html.parser
* [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)

#### 实战：通过用户输入词条和数量，爬取百度百科词条相关内容
#### 目录结构：

```
|-- Baike_spider（文件夹）
   |-- spider_main.py（爬虫调度器————程序入口，管理各个模块）
   |-- url_manager.py（URL 管理器————管理 url）
   |-- html_downloader.py （网页下载器————通过url获取网页内容）
   |-- html_parser.py （网页解析器————通过网页内容解析出新的 url 和 新的内容）
   |-- html_outputer.py （输出————将获取到的数据输出）
```

##### 用到的知识点：

* 使用 urllib 请求数据
* 使用 BeautifulSoup 解析网页
* 使用 re 正则匹配 url 数据
* 使用 urljoin 将拼接 url

